{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Amazon SageMaker HuggingFace and Hyperparameter Tuning\n",
    "\n",
    "Automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. You choose the tunable hyperparameters, a range of values for each, and an objective metric. You choose the objective metric from the metrics that the algorithm computes. Automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Text Classification can be used to solve various use-cases like sentiment analysis, spam detection, hashtag prediction etc. \n",
    "\n",
    "\n",
    "This notebook demonstrates the use of the [HuggingFace `transformers` library](https://huggingface.co/transformers/) together with a custom Amazon sagemaker-sdk extension to fine-tune a pre-trained transformer on multi class text classification. In particular, the pre-trained model will be fine-tuned using the [`20 newsgroups dataset`](http://qwone.com/~jason/20Newsgroups/). To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: scikit_learn>=1.0.2 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (1.0.2)\n",
      "Requirement already satisfied: sagemaker>=2.82.1 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (2.82.1)\n",
      "Requirement already satisfied: transformers>=4.18.0 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (4.18.0)\n",
      "Requirement already satisfied: datasets[s3]>=1.18.2 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (1.18.3)\n",
      "Requirement already satisfied: nltk>=3.7 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from scikit_learn>=1.0.2) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from scikit_learn>=1.0.2) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from scikit_learn>=1.0.2) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from scikit_learn>=1.0.2) (1.20.3)\n",
      "Requirement already satisfied: google-pasta in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from sagemaker>=2.82.1) (0.2.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from sagemaker>=2.82.1) (0.1.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from sagemaker>=2.82.1) (3.19.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from sagemaker>=2.82.1) (4.8.2)\n",
      "Requirement already satisfied: pathos in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from sagemaker>=2.82.1) (0.2.8)\n",
      "Requirement already satisfied: boto3>=1.20.21 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from sagemaker>=2.82.1) (1.21.11)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from sagemaker>=2.82.1) (21.3)\n",
      "Requirement already satisfied: attrs==20.3.0 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from sagemaker>=2.82.1) (20.3.0)\n",
      "Requirement already satisfied: pandas in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from sagemaker>=2.82.1) (1.4.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from sagemaker>=2.82.1) (1.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from transformers>=4.18.0) (5.4.1)\n",
      "Requirement already satisfied: sacremoses in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from transformers>=4.18.0) (0.0.44)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from transformers>=4.18.0) (0.11.6)\n",
      "Requirement already satisfied: requests in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from transformers>=4.18.0) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from transformers>=4.18.0) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from transformers>=4.18.0) (2021.9.30)\n",
      "Requirement already satisfied: filelock in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from transformers>=4.18.0) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from transformers>=4.18.0) (0.4.0)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from datasets[s3]>=1.18.2) (4.0.1)\n",
      "Requirement already satisfied: dill in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from datasets[s3]>=1.18.2) (0.3.4)\n",
      "Requirement already satisfied: aiohttp in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from datasets[s3]>=1.18.2) (3.7.4.post0)\n",
      "Requirement already satisfied: multiprocess in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from datasets[s3]>=1.18.2) (0.70.12.2)\n",
      "Requirement already satisfied: xxhash in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from datasets[s3]>=1.18.2) (2.0.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from datasets[s3]>=1.18.2) (2022.1.0)\n",
      "Requirement already satisfied: s3fs in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from datasets[s3]>=1.18.2) (0.4.2)\n",
      "Requirement already satisfied: botocore in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from datasets[s3]>=1.18.2) (1.24.11)\n",
      "Requirement already satisfied: click in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from nltk>=3.7) (7.1.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from boto3>=1.20.21->sagemaker>=2.82.1) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from boto3>=1.20.21->sagemaker>=2.82.1) (0.5.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from botocore->datasets[s3]>=1.18.2) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from botocore->datasets[s3]>=1.18.2) (1.26.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=4.18.0) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from importlib-metadata>=1.4.0->sagemaker>=2.82.1) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from packaging>=20.0->sagemaker>=2.82.1) (3.0.4)\n",
      "Requirement already satisfied: six in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker>=2.82.1) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from requests->transformers>=4.18.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from requests->transformers>=4.18.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from requests->transformers>=4.18.0) (2.0.4)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from aiohttp->datasets[s3]>=1.18.2) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from aiohttp->datasets[s3]>=1.18.2) (5.1.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from aiohttp->datasets[s3]>=1.18.2) (4.0.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from aiohttp->datasets[s3]>=1.18.2) (1.6.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from pandas->sagemaker>=2.82.1) (2021.3)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from pathos->sagemaker>=2.82.1) (0.3.0)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/data2/alfred/anaconda3/envs/aws/lib/python3.8/site-packages (from pathos->sagemaker>=2.82.1) (1.6.6.4)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install \"scikit_learn>=1.0.2\" \"sagemaker>=2.82.1\" \"transformers>=4.18.0\" \"datasets[s3]>=1.18.2\" \"nltk>=3.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this notebook in SageMaker Studio, you need to make sure `ipywidgets` is installed and restart the kernel, so please uncomment the code in the next cell, and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install ipywidgets\n",
    "IPython.Application.instance().kernel.do_shutdown(True)  # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. If you don't specify a bucket, SageMaker SDK will create a default bucket following a pre-defined naming convention in the same region. \n",
    "- The IAM role ARN used to give SageMaker access to your data. It can be fetched using the **get_execution_role** method from sagemaker python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::976939723775:role/service-role/AmazonSageMaker-ExecutionRole-20210317T133000\n",
      "sagemaker-us-west-2-976939723775\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sagemaker.huggingface\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "print(\n",
    "    role\n",
    ")  # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = sess.default_bucket()  # Replace with your own bucket name if needed\n",
    "print(bucket)\n",
    "s3_prefix = \"huggingface/20_newsgroups\"  # Replace with the prefix under which you want to store the data if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Now we'll download a dataset from the web on which we want to train the text classification model.\n",
    "\n",
    "In this example, let us train the text classification model on the [`20 newsgroups dataset`](http://qwone.com/~jason/20Newsgroups/). The `20 newsgroups dataset` consists of 20000 messages taken from 20 Usenet newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "data_dir = \"20_newsgroups_bulk\"\n",
    "if os.path.exists(data_dir):  # cleanup existing data folder\n",
    "    shutil.rmtree(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-sample-files/datasets/text/20_newsgroups/20_newsgroups_bulk.tar.gz to ./20_newsgroups_bulk.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://sagemaker-sample-files/datasets/text/20_newsgroups/20_newsgroups_bulk.tar.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\t\t  rec.autos\t      sci.space\n",
      "comp.graphics\t\t  rec.motorcycles     soc.religion.christian\n",
      "comp.os.ms-windows.misc   rec.sport.baseball  talk.politics.guns\n",
      "comp.sys.ibm.pc.hardware  rec.sport.hockey    talk.politics.mideast\n",
      "comp.sys.mac.hardware\t  sci.crypt\t      talk.politics.misc\n",
      "comp.windows.x\t\t  sci.electronics     talk.religion.misc\n",
      "misc.forsale\t\t  sci.med\n"
     ]
    }
   ],
   "source": [
    "!tar xzf 20_newsgroups_bulk.tar.gz\n",
    "!ls 20_newsgroups_bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 20\n"
     ]
    }
   ],
   "source": [
    "file_list = [os.path.join(data_dir, f) for f in os.listdir(data_dir)]\n",
    "print(\"Number of files:\", len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 19997\n"
     ]
    }
   ],
   "source": [
    "documents_count = 0\n",
    "for file in file_list:\n",
    "    df = pd.read_csv(file, header=None, names=[\"text\"])\n",
    "    documents_count = documents_count + df.shape[0]\n",
    "print(\"Number of documents:\", documents_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the dataset files and analyze the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_list = [f.split(\"/\")[1] for f in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['talk.politics.misc',\n",
       " 'rec.autos',\n",
       " 'rec.sport.baseball',\n",
       " 'alt.atheism',\n",
       " 'sci.space',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'comp.graphics',\n",
       " 'rec.sport.hockey',\n",
       " 'talk.politics.mideast',\n",
       " 'sci.crypt',\n",
       " 'talk.politics.guns',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'misc.forsale',\n",
       " 'sci.electronics',\n",
       " 'soc.religion.christian',\n",
       " 'rec.motorcycles',\n",
       " 'sci.med',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset consists of 20 topics, each in different file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the dataset to get some understanding about how the data and the label is provided in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Newsgroups: rec.motorcycles\\nPath: cantaloupe....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Newsgroups: rec.motorcycles\\nPath: cantaloupe....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Newsgroups: rec.motorcycles\\nPath: cantaloupe....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Newsgroups: rec.motorcycles\\nPath: cantaloupe....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!rochester!corn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Newsgroups: rec.motorcycles\\nPath: cantaloupe....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Newsgroups: rec.motorcycles\\nPath: cantaloupe....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0    Newsgroups: rec.motorcycles\\nPath: cantaloupe....\n",
       "1    Newsgroups: rec.motorcycles\\nPath: cantaloupe....\n",
       "2    Newsgroups: rec.motorcycles\\nPath: cantaloupe....\n",
       "3    Newsgroups: rec.motorcycles\\nPath: cantaloupe....\n",
       "4    Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...\n",
       "..                                                 ...\n",
       "995  Path: cantaloupe.srv.cs.cmu.edu!rochester!corn...\n",
       "996  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...\n",
       "997  Newsgroups: rec.motorcycles\\nPath: cantaloupe....\n",
       "998  Newsgroups: rec.motorcycles\\nPath: cantaloupe....\n",
       "999  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./20_newsgroups_bulk/rec.motorcycles\", header=None, names=[\"text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Newsgroups: rec.motorcycles\\nPath: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!fs7.ece.cmu.edu!europa.eng.gtefsd.com!howland.reston.ans.net!agate!linus!linus.mitre.org!mbunix.mitre.org!cookson\\nFrom: cookson@mbunix.mitre.org (Cookson)\\nSubject: Volvo Attack!\\nMessage-ID: <1993Apr21.143403.4644@linus.mitre.org>\\nSender: news@linus.mitre.org (News Service)\\nNntp-Posting-Host: mbunix.mitre.org\\nOrganization: The MITRE Corp., Bedford, Ma.\\nDate: Wed, 21 Apr 1993 14:34:03 GMT\\nLines: 22\\n\\nI was privelged enough to experience my first Volvo attack this weekend.\\n\\nI was last in a line of traffic that was about 6 vehicles long, riding\\ndown Rt. 40 in Groton Ma.  At the side of the road, sitting off on the\\nshoulder was the killer Volvo in question.  No brake lights, no turn signal,\\nnothing.  We were doing about 40 mph and I was following the cage in front\\nof me about 2.5-3 sec. back.  Well, as said cage passes the Volvo, the\\nBrain Dead Idiot (tm) behind the wheel decides that she doesn\\'t need to wait\\nfor me to pass also and turns out perpendicular across both lanes of traffic\\nso that she can turn around...  So I get on the brakes in a effort to not\\nT-bone it, and the horn in an effort to wake the BDI up.  As she finishes\\nthe turn, she looks up at me with a completely blank, uncomprehending\\nstare.\\n\\nWhere can I get rocket launchers for the VFR?\\n\\nDean\\n-- \\n| Dean Cookson / dcookson@mitre.org / 617 271-2714    | DoD #207  AMA #573534 |\\n| The MITRE Corp. Burlington Rd., Bedford, Ma. 01730  | KotNML  /  KotB       |\\n| \"The road is my shepherd and I shall not stop\"      | \\'92 VFR750F           |\\n| -Sam Eliott, Road Hogs MTV 1993                     | \\'88 Bianchi Limited   |\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Newsgroups: comp.sys.mac.hardware\\nPath: canta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Newsgroups: comp.sys.mac.hardware\\nPath: canta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newsgroups: comp.sys.mac.hardware\\nPath: canta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Newsgroups: comp.sys.mac.hardware\\nPath: canta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu comp.sys.ibm.p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Newsgroups: comp.sys.mac.hardware\\nPath: canta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Newsgroups: comp.sys.mac.hardware\\nPath: canta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0    Newsgroups: comp.sys.mac.hardware\\nPath: canta...\n",
       "1    Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....\n",
       "2    Newsgroups: comp.sys.mac.hardware\\nPath: canta...\n",
       "3    Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...\n",
       "4    Newsgroups: comp.sys.mac.hardware\\nPath: canta...\n",
       "..                                                 ...\n",
       "995  Newsgroups: comp.sys.mac.hardware\\nPath: canta...\n",
       "996  Xref: cantaloupe.srv.cs.cmu.edu comp.sys.ibm.p...\n",
       "997  Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....\n",
       "998  Newsgroups: comp.sys.mac.hardware\\nPath: canta...\n",
       "999  Newsgroups: comp.sys.mac.hardware\\nPath: canta...\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./20_newsgroups_bulk/comp.sys.mac.hardware\", header=None, names=[\"text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Newsgroups: comp.sys.mac.hardware\\nPath: cantaloupe.srv.cs.cmu.edu!rochester!udel!gatech!howland.reston.ans.net!agate!headwall.Stanford.EDU!nntp.Stanford.EDU!cmwand\\nFrom: cmwand@leland.Stanford.EDU (Christopher Wand)\\nSubject: Re: Syquest 150 ???\\nMessage-ID: <1993Apr20.043629.21237@leland.Stanford.EDU>\\nSender: news@leland.Stanford.EDU (Mr News)\\nOrganization: DSG, Stanford University, CA 94305, USA\\nReferences: <93759@hydra.gatech.EDU>\\nDistribution: usa\\nDate: Tue, 20 Apr 93 04:36:29 GMT\\nLines: 30\\n\\nIn article <93759@hydra.gatech.EDU> gt8798a@prism.gatech.EDU (Anthony S. Kim) writes:\\n>I remember someone mention about a 150meg syquest.  Has anyone else\\n>heard anything about this?  I\\'d be interested in the cost per megabyte and the\\n>approximate cost of the drive itself and how they compare to the Bernoulli 150.\\n\\nI think you must be talking about the Syquest 105 (code named Mesa I believe).\\nIt is a 3.5\" Winchester technology drive pretty much like the other Syquest\\ndrives in terms of how it works. According to the latest MacLeak, the \\ndrive has a 14.5 ms access time, 1.9 MB/s sustained throughput (these figures\\nare from memory so they could be slightly off, but they give you an idea of\\nperformance nonetheless). The drive was originally released for the PC\\nand just recently was released for the Mac world (don\\'t ask me what the \\ndifferences are) and through they are currently in limited supply, according\\nto a Syquest rep. they are in the process of ramping up for mass production.\\nI have already seen them advertised by a number of manufacturers in MacLeak\\nincluding PLI, MassMicro, ClubMac, and MacWarehouse\\'s PowerUser. The PLI\\nand MassMicro units are priced at just around $1000; the lesser name brands\\nare going for around $750 for an external drive. Cartridges which hold \\n105 MB sell for about $80 each. At these prices, the drives and cartridges\\nare cheaper and better performing than the 88MB drives.\\nCost per megabyte compares favorably with other cartridge drives and Bernoulli\\ndrives, but for large amounts of data optical is still cheaper, and more\\nreliable.  Personally, I\\'m excited by the new drive and look forward to \\ngetting my hands on one.\\n-Chris Wand\\n\\n-- \\n\\n\"Egotism is the anesthetic that dulls the pain of stupidity.\"\\n                                                     - Frank Leahy\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above, there is a single file for each class in the dataset. Each record is just a plain text paragraphs with header, body, footer and quotes. We will need to process them into a suitable data format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We need to preprocess the dataset to remove the header, footer, quotes, leading/trailing whitespace, extra spaces, tabs, and HTML tags/markups. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `nltk` tokenizer and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alfred/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets._twenty_newsgroups import (\n",
    "    strip_newsgroup_header,\n",
    "    strip_newsgroup_quoting,\n",
    "    strip_newsgroup_footer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following function will remove the header, footer and quotes (of earlier messages in each text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_newsgroup_item(item):\n",
    "    item = strip_newsgroup_header(item)\n",
    "    item = strip_newsgroup_quoting(item)\n",
    "    item = strip_newsgroup_footer(item)\n",
    "    return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will take care of removing leading/trailing whitespace, extra spaces, tabs, and HTML tags/markups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(texts):\n",
    "    final_text_list = []\n",
    "    for text in texts:\n",
    "\n",
    "        # Check if the sentence is a missing value\n",
    "        if isinstance(text, str) == False:\n",
    "            text = \"\"\n",
    "\n",
    "        filtered_sentence = []\n",
    "\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove leading/trailing whitespace, extra space, tabs, and HTML tags/markups\n",
    "        text = text.strip()\n",
    "        text = re.sub(\"\\[.*?\\]\", \"\", text)\n",
    "        text = re.sub(\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "        text = re.sub(\"<.*?>+\", \"\", text)\n",
    "        text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
    "        text = re.sub(\"\\n\", \"\", text)\n",
    "        text = re.sub(\"\\w*\\d\\w*\", \"\", text)\n",
    "\n",
    "        for w in word_tokenize(text):\n",
    "            # We are applying some custom filtering here, feel free to try different things\n",
    "            # Check if it is not numeric\n",
    "            if not w.isnumeric():\n",
    "                filtered_sentence.append(w)\n",
    "        final_string = \" \".join(filtered_sentence)  # final string of cleaned words\n",
    "\n",
    "        final_text_list.append(final_string)\n",
    "\n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will read each of the `20_newsgroups` dataset files, call `strip_newsgroup_item` and `process_text` functions we defined earlier, and then aggregate all data into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/talk.politics.misc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/rec.autos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/rec.sport.baseball\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/alt.atheism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/sci.space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/comp.sys.mac.hardware\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/comp.windows.x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/comp.graphics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/rec.sport.hockey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/talk.politics.mideast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/sci.crypt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/talk.politics.guns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/comp.os.ms-windows.misc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/comp.sys.ibm.pc.hardware\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/misc.forsale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/sci.electronics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/soc.religion.christian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/rec.motorcycles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/sci.med\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20_newsgroups_bulk/talk.religion.misc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-163c8b3745cc>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_categories_df = all_categories_df.append(df, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "all_categories_df = pd.DataFrame()\n",
    "\n",
    "for file in file_list:\n",
    "    print(f\"Processing {file}\")\n",
    "    label = file.split(\"/\")[1]\n",
    "    df = pd.read_csv(file, header=None, names=[\"text\"])\n",
    "    df[\"text\"] = df[\"text\"].apply(strip_newsgroup_item)\n",
    "    df[\"text\"] = process_text(df[\"text\"].tolist())\n",
    "    df[\"label\"] = label\n",
    "    all_categories_df = all_categories_df.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect how many categories there are in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "talk.politics.misc          1000\n",
       "rec.autos                   1000\n",
       "sci.med                     1000\n",
       "rec.motorcycles             1000\n",
       "sci.electronics             1000\n",
       "misc.forsale                1000\n",
       "comp.sys.ibm.pc.hardware    1000\n",
       "comp.os.ms-windows.misc     1000\n",
       "talk.politics.guns          1000\n",
       "sci.crypt                   1000\n",
       "talk.politics.mideast       1000\n",
       "rec.sport.hockey            1000\n",
       "comp.graphics               1000\n",
       "comp.windows.x              1000\n",
       "comp.sys.mac.hardware       1000\n",
       "sci.space                   1000\n",
       "alt.atheism                 1000\n",
       "rec.sport.baseball          1000\n",
       "talk.religion.misc          1000\n",
       "soc.religion.christian       997\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_categories_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset there are 20 categories which is too much, so we will combine the sub-categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace to politics\n",
    "all_categories_df[\"label\"].replace(\n",
    "    {\n",
    "        \"talk.politics.misc\": \"politics\",\n",
    "        \"talk.politics.guns\": \"politics\",\n",
    "        \"talk.politics.mideast\": \"politics\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# replace to recreational\n",
    "all_categories_df[\"label\"].replace(\n",
    "    {\n",
    "        \"rec.sport.hockey\": \"recreational\",\n",
    "        \"rec.sport.baseball\": \"recreational\",\n",
    "        \"rec.autos\": \"recreational\",\n",
    "        \"rec.motorcycles\": \"recreational\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# replace to religion\n",
    "all_categories_df[\"label\"].replace(\n",
    "    {\n",
    "        \"soc.religion.christian\": \"religion\",\n",
    "        \"talk.religion.misc\": \"religion\",\n",
    "        \"alt.atheism\": \"religion\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# replace to computer\n",
    "all_categories_df[\"label\"].replace(\n",
    "    {\n",
    "        \"comp.windows.x\": \"computer\",\n",
    "        \"comp.sys.ibm.pc.hardware\": \"computer\",\n",
    "        \"comp.os.ms-windows.misc\": \"computer\",\n",
    "        \"comp.graphics\": \"computer\",\n",
    "        \"comp.sys.mac.hardware\": \"computer\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "# replace to sales\n",
    "all_categories_df[\"label\"].replace({\"misc.forsale\": \"sales\"}, inplace=True)\n",
    "\n",
    "# replace to science\n",
    "all_categories_df[\"label\"].replace(\n",
    "    {\n",
    "        \"sci.crypt\": \"science\",\n",
    "        \"sci.electronics\": \"science\",\n",
    "        \"sci.med\": \"science\",\n",
    "        \"sci.space\": \"science\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are left with 6 categories, which is much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "computer        5000\n",
       "recreational    4000\n",
       "science         4000\n",
       "politics        3000\n",
       "religion        2997\n",
       "sales           1000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_categories_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate number of words for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>too many</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when mcmanus says we have the worlds best medi...</td>\n",
       "      <td>politics</td>\n",
       "      <td>819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im addicted to chocolate myself</td>\n",
       "      <td>politics</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wow does this mean out of homosexuals will be ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if you cant convict em dont bust em plea bargi...</td>\n",
       "      <td>politics</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label  word_count\n",
       "0                                           too many  politics           2\n",
       "1  when mcmanus says we have the worlds best medi...  politics         819\n",
       "2                    im addicted to chocolate myself  politics           5\n",
       "3  wow does this mean out of homosexuals will be ...  politics          26\n",
       "4  if you cant convict em dont bust em plea bargi...  politics          18"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_categories_df[\"word_count\"] = all_categories_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "all_categories_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get basic statistics about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    19997.000000\n",
       "mean       159.346102\n",
       "std        434.479067\n",
       "min          0.000000\n",
       "25%         37.000000\n",
       "50%         74.000000\n",
       "75%        148.000000\n",
       "max      11351.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_categories_df[\"word_count\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean value is around 159 words. However, there are outliers, such as a text with 11351 words. This can make it harder for the model to result in good performance. We will take care to drop those rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop empty rows first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "no_text = all_categories_df[all_categories_df[\"word_count\"] == 0]\n",
    "print(len(no_text))\n",
    "\n",
    "# drop these rows\n",
    "all_categories_df.drop(no_text.index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the rows that are longer than 256 words, as it is a length close to the mean value of the word count. This is done to make it easy for the model to train without outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2409\n"
     ]
    }
   ],
   "source": [
    "long_text = all_categories_df[all_categories_df[\"word_count\"] > 256]\n",
    "print(len(long_text))\n",
    "\n",
    "# drop these rows\n",
    "all_categories_df.drop(long_text.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "computer        4659\n",
       "recreational    3675\n",
       "science         3506\n",
       "politics        2370\n",
       "religion        2349\n",
       "sales            939\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_categories_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get basic statistics about the dataset after our outliers fixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    17498.000000\n",
       "mean        79.797348\n",
       "std         59.636188\n",
       "min          1.000000\n",
       "25%         33.000000\n",
       "50%         64.000000\n",
       "75%        113.000000\n",
       "max        256.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_categories_df[\"word_count\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much more balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we drop the `word_count` columns as we will not need it anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_categories_df.drop(columns=\"word_count\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>too many</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im addicted to chocolate myself</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wow does this mean out of homosexuals will be ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if you cant convict em dont bust em plea bargi...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>so it will be interesting to see the reaction ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991</th>\n",
       "      <td>this is cute but i see no statement telling me...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19992</th>\n",
       "      <td>im confused could you restate what yer saying ...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>not true consider the case of a coin i flip it...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>contradicting itself on facts for example</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>in god whose word i praise in god i trust i wi...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17498 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text     label\n",
       "0                                               too many  politics\n",
       "2                        im addicted to chocolate myself  politics\n",
       "3      wow does this mean out of homosexuals will be ...  politics\n",
       "4      if you cant convict em dont bust em plea bargi...  politics\n",
       "5      so it will be interesting to see the reaction ...  politics\n",
       "...                                                  ...       ...\n",
       "19991  this is cute but i see no statement telling me...  religion\n",
       "19992  im confused could you restate what yer saying ...  religion\n",
       "19994  not true consider the case of a coin i flip it...  religion\n",
       "19995          contradicting itself on facts for example  religion\n",
       "19996  in god whose word i praise in god i trust i wi...  religion\n",
       "\n",
       "[17498 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_categories_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert categorical label to integer number, in order to prepare the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['politics', 'recreational', 'religion', 'science', 'computer', 'sales']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = all_categories_df[\"label\"].unique().tolist()\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories.index(\"recreational\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_categories_df[\"label\"] = all_categories_df[\"label\"].apply(lambda x: categories.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    4659\n",
       "1    3675\n",
       "3    3506\n",
       "0    2370\n",
       "2    2349\n",
       "5     939\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_categories_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We partition the dataset into 80% training and 20% validation set and save to `csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(all_categories_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the label distribution in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    3735\n",
       "1    2936\n",
       "3    2767\n",
       "0    1921\n",
       "2    1880\n",
       "5     759\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the label distribution in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    924\n",
       "3    739\n",
       "1    739\n",
       "2    469\n",
       "0    449\n",
       "5    180\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer is in charge of preparing the inputs for a model. The library contains tokenizers for all the models. Most of the tokenizers are available in two flavors: a full python implementation and a “Fast” implementation based on the Rust library [tokenizers](https://github.com/huggingface/tokenizers). The “Fast” implementations allows:\n",
    "\n",
    " - A significant speed-up in particular when doing batched tokenization.\n",
    " - Additional methods to map between the original string (character and words) and the token space (e.g. getting the index of the token comprising a given character or the span of characters corresponding to a given token). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7e370caa034e34a872658eb303f2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e020aad2184898909c5ec582fafa8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91da7bd3804451684fb4dba0e2c5086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ab6e435643424e8700356bd9401a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test datasets\n",
    "\n",
    "Let's create a [Dataset](https://huggingface.co/docs/datasets/loading_datasets.html) from our local `csv` files for training and test we saved earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-54a3fef1df061eb4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/alfred/.cache/huggingface/datasets/csv/default-54a3fef1df061eb4/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84283b3abde43898dd36347ddd7f9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9f962b78964b868499bc66742bd52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/alfred/.cache/huggingface/datasets/csv/default-54a3fef1df061eb4/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f61e9ad9e224a5599dc9f6d324de610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"test\": \"test.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 13998\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 3500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 13998\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'in win one may assign hotkeys for the program items within theprogram manager how about the program manager itself is there onealready or is there some way to assign one',\n",
       " 'label': 4}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 3500\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i dont know if it causes the body any harm but in the ive been teaching nine and ten years olds ive never hadone fall over from eating boogers which many kids do on aregular basis',\n",
       " 'label': 3}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize train and test datasets\n",
    "\n",
    "Let's tokenize the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae9481941094d05bbe7e4b53afd4630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e44487fb2ac47369910f955c48b1f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = test_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set format for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "After we processed the datasets, we are going to upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-976939723775/huggingface/20_newsgroups/train\n",
      "s3://sagemaker-us-west-2-976939723775/huggingface/20_newsgroups/test\n"
     ]
    }
   ],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/train\"\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/test\"\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)\n",
    "\n",
    "print(training_input_path)\n",
    "print(test_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up hyperparameter tuning job\n",
    "Now that we are done with all the setup that is needed, we are ready to train our HuggingFace model. To begin, let us create a `HuggingFace` estimator object. This estimator will launch the training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the HuggingFace model for supervised text classification\n",
    "\n",
    "In order to create a sagemaker training job we need a `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In an Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./code',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            volume_size=256,\n",
    "                            role=role,\n",
    "                            transformers_version='4.6',\n",
    "                            pytorch_version='1.7',\n",
    "                            py_version='py36',\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'model_name':'distilbert-base-uncased',\n",
    "                                               'num_labels': 6\n",
    "                                              })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --num_labels 6\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. \n",
    "\n",
    "SageMaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local-gpu'` for `gpu` usage. _Note: this does not work within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a metric_definition dictionary that contains regex-based definitions that will be used to parse the job logs and extract metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {\"Name\": \"loss\", \"Regex\": \"'loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {\"Name\": \"learning_rate\", \"Regex\": \"'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {\"Name\": \"eval_loss\", \"Regex\": \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {\"Name\": \"eval_accuracy\", \"Regex\": \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {\"Name\": \"eval_f1\", \"Regex\": \"'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {\"Name\": \"eval_precision\", \"Regex\": \"'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {\"Name\": \"eval_recall\", \"Regex\": \"'eval_recall': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {\"Name\": \"eval_runtime\", \"Regex\": \"'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {\n",
    "        \"Name\": \"eval_samples_per_second\",\n",
    "        \"Regex\": \"'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?\",\n",
    "    },\n",
    "    {\"Name\": \"epoch\", \"Regex\": \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\"epochs\": 1, \"model_name\": \"distilbert-base-uncased\", \"num_labels\": 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the SageMaker `HuggingFace` estimator with resource configurations and hyperparameters to train Text Classification on `20 newsgroups` dataset, running on a `p3.2xlarge` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"./code\",\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    instance_count=1,\n",
    "    volume_size=256,\n",
    "    role=role,\n",
    "    transformers_version=\"4.6\",\n",
    "    pytorch_version=\"1.7\",\n",
    "    py_version=\"py36\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've defined our estimator we can specify the hyperparameters we'd like to tune and their possible values.  We have three different types of hyperparameters.\n",
    "- Categorical parameters need to take one value from a discrete set.  We define this by passing the list of possible values to `CategoricalParameter(list)`\n",
    "- Continuous parameters can take any real number value between the minimum and maximum value, defined by `ContinuousParameter(min, max)`\n",
    "- Integer parameters can take any integer value between the minimum and maximum value, defined by `IntegerParameter(min, max)`\n",
    "\n",
    "*Note, if possible, it's almost always best to specify a value as the least restrictive type.  For example, tuning learning rate as a continuous value between 0.01 and 0.2 is likely to yield a better result than tuning as a categorical parameter with values 0.01, 0.1, 0.15, or 0.2.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"train_batch_size\": IntegerParameter(8, 32),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll specify the objective metric that we'd like to tune and its definition, which includes the regular expression (Regex) needed to extract that metric from the CloudWatch logs of the training job.  If you bring your own algorithm, your algorithm emits metrics by itself. In that case, you'll need to add a `MetricDefinition` object here to define the format of those metrics through regex, so that SageMaker knows how to extract those metrics from your CloudWatch logs.\n",
    "\n",
    "In this case, we elected to monitor `eval_accuracy` as you can see below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = \"eval_accuracy\"\n",
    "objective_type = \"Maximize\"\n",
    "hpo_metric_definitions = [\n",
    "    {\"Name\": \"eval_accuracy\", \"Regex\": \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a `HyperparameterTuner` object, to which we pass:\n",
    "- The `HuggingFace` estimator we created above\n",
    "- Our hyperparameter ranges\n",
    "- Objective metric name and definition\n",
    "- Tuning resource configurations such as Number of training jobs to run in total and how many training jobs can be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    huggingface_estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    hpo_metric_definitions,\n",
    "    max_jobs=6,\n",
    "    max_parallel_jobs=3,\n",
    "    objective_type=objective_type,\n",
    "    strategy='Bayesian', #Strategy to be used for hyperparameter estimations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch hyperparameter tuning job\n",
    "Now we can launch a hyperparameter tuning job by calling *fit()* function. After the hyperparameter tuning job is created, we can go to SageMaker console to track the progress of the hyperparameter tuning job until it is completed.\n",
    "\n",
    "This should take around 28 minutes to complete. Switch to lab while wating for the fine tuning job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................................................................................................................................................................................................................................................................................................................................!\n",
      "CPU times: user 1.66 s, sys: 229 ms, total: 1.88 s\n",
      "Wall time: 29min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tuner.fit({\"train\": training_input_path, \"test\": test_input_path}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results of a Hyperparameter Tuning job\n",
    "\n",
    "Once you have completed a tuning job, (or even while the job is still running) you can use the code below to analyze the results to understand how each hyperparameter effects the quality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-pytorch--220510-2100'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client = boto3.Session().client(\"sagemaker\")\n",
    "\n",
    "tuning_job_name = tuner.latest_tuning_job.name\n",
    "tuning_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track hyperparameter tuning job progress\n",
    "After you launch a tuning job, you can see its progress by calling `describe_tuning_job` API. The output from describe-tuning-job is a JSON object that contains information about the current state of the tuning job. You can call `list_training_jobs_for_tuning_job` to see a detailed list of the training jobs that the tuning job launched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 training jobs have completed\n"
     ]
    }
   ],
   "source": [
    "tuning_job_result = sm_client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name\n",
    ")\n",
    "\n",
    "status = tuning_job_result[\"HyperParameterTuningJobStatus\"]\n",
    "if status != \"Completed\":\n",
    "    print(\"Reminder: the tuning job has not been completed.\")\n",
    "\n",
    "job_count = tuning_job_result[\"TrainingJobStatusCounters\"][\"Completed\"]\n",
    "print(\"%d training jobs have completed\" % job_count)\n",
    "\n",
    "is_minimize = (\n",
    "    tuning_job_result[\"HyperParameterTuningJobConfig\"][\"HyperParameterTuningJobObjective\"][\"Type\"]\n",
    "    != \"Maximize\"\n",
    ")\n",
    "objective_name = tuning_job_result[\"HyperParameterTuningJobConfig\"][\n",
    "    \"HyperParameterTuningJobObjective\"\n",
    "][\"MetricName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model found so far:\n",
      "{'CreationTime': datetime.datetime(2022, 5, 10, 21, 0, 15, tzinfo=tzlocal()),\n",
      " 'FinalHyperParameterTuningJobObjectiveMetric': {'MetricName': 'eval_accuracy',\n",
      "                                                 'Value': 0.8057143092155457},\n",
      " 'ObjectiveStatus': 'Succeeded',\n",
      " 'TrainingEndTime': datetime.datetime(2022, 5, 10, 21, 12, 40, tzinfo=tzlocal()),\n",
      " 'TrainingJobArn': 'arn:aws:sagemaker:us-west-2:976939723775:training-job/huggingface-pytorch--220510-2100-002-a51ba0eb',\n",
      " 'TrainingJobName': 'huggingface-pytorch--220510-2100-002-a51ba0eb',\n",
      " 'TrainingJobStatus': 'Completed',\n",
      " 'TrainingStartTime': datetime.datetime(2022, 5, 10, 21, 2, 1, tzinfo=tzlocal()),\n",
      " 'TunedHyperParameters': {'train_batch_size': '15'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "if tuning_job_result.get(\"BestTrainingJob\", None):\n",
    "    print(\"Best model found so far:\")\n",
    "    pprint(tuning_job_result[\"BestTrainingJob\"])\n",
    "else:\n",
    "    print(\"No training jobs have reported results yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch all results as `DataFrame`\n",
    "We can list hyperparameters and objective metrics of all training jobs and pick up the training job with the best objective metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training jobs with valid objective: 6\n",
      "{'lowest': 0.7548571228981018, 'highest': 0.8057143092155457}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-72-c0ae9ee1f170>:13: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option(\"display.max_colwidth\", -1)  # Don't truncate TrainingJobName\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_batch_size</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.0</td>\n",
       "      <td>huggingface-pytorch--220510-2100-002-a51ba0eb</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.805714</td>\n",
       "      <td>2022-05-10 21:02:01+00:00</td>\n",
       "      <td>2022-05-10 21:12:40+00:00</td>\n",
       "      <td>639.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>huggingface-pytorch--220510-2100-006-82e267e1</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.792286</td>\n",
       "      <td>2022-05-10 21:16:33+00:00</td>\n",
       "      <td>2022-05-10 21:26:16+00:00</td>\n",
       "      <td>583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.0</td>\n",
       "      <td>huggingface-pytorch--220510-2100-003-f681625a</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>2022-05-10 21:02:10+00:00</td>\n",
       "      <td>2022-05-10 21:12:45+00:00</td>\n",
       "      <td>635.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.0</td>\n",
       "      <td>huggingface-pytorch--220510-2100-005-b806ad96</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.785429</td>\n",
       "      <td>2022-05-10 21:16:30+00:00</td>\n",
       "      <td>2022-05-10 21:26:54+00:00</td>\n",
       "      <td>624.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.0</td>\n",
       "      <td>huggingface-pytorch--220510-2100-004-f3c72f5a</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.777714</td>\n",
       "      <td>2022-05-10 21:16:26+00:00</td>\n",
       "      <td>2022-05-10 21:27:01+00:00</td>\n",
       "      <td>635.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.0</td>\n",
       "      <td>huggingface-pytorch--220510-2100-001-82213142</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.754857</td>\n",
       "      <td>2022-05-10 21:02:01+00:00</td>\n",
       "      <td>2022-05-10 21:12:36+00:00</td>\n",
       "      <td>635.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_batch_size                                TrainingJobName  \\\n",
       "4  15.0              huggingface-pytorch--220510-2100-002-a51ba0eb   \n",
       "0  18.0              huggingface-pytorch--220510-2100-006-82e267e1   \n",
       "3  22.0              huggingface-pytorch--220510-2100-003-f681625a   \n",
       "1  25.0              huggingface-pytorch--220510-2100-005-b806ad96   \n",
       "2  16.0              huggingface-pytorch--220510-2100-004-f3c72f5a   \n",
       "5  10.0              huggingface-pytorch--220510-2100-001-82213142   \n",
       "\n",
       "  TrainingJobStatus  FinalObjectiveValue         TrainingStartTime  \\\n",
       "4  Completed         0.805714            2022-05-10 21:02:01+00:00   \n",
       "0  Completed         0.792286            2022-05-10 21:16:33+00:00   \n",
       "3  Completed         0.790000            2022-05-10 21:02:10+00:00   \n",
       "1  Completed         0.785429            2022-05-10 21:16:30+00:00   \n",
       "2  Completed         0.777714            2022-05-10 21:16:26+00:00   \n",
       "5  Completed         0.754857            2022-05-10 21:02:01+00:00   \n",
       "\n",
       "            TrainingEndTime  TrainingElapsedTimeSeconds  \n",
       "4 2022-05-10 21:12:40+00:00  639.0                       \n",
       "0 2022-05-10 21:26:16+00:00  583.0                       \n",
       "3 2022-05-10 21:12:45+00:00  635.0                       \n",
       "1 2022-05-10 21:26:54+00:00  624.0                       \n",
       "2 2022-05-10 21:27:01+00:00  635.0                       \n",
       "5 2022-05-10 21:12:36+00:00  635.0                       "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tuner_analytics = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "\n",
    "full_df = tuner_analytics.dataframe()\n",
    "\n",
    "if len(full_df) > 0:\n",
    "    df = full_df[full_df[\"FinalObjectiveValue\"] > -float(\"inf\")]\n",
    "    if len(df) > 0:\n",
    "        df = df.sort_values(\"FinalObjectiveValue\", ascending=is_minimize)\n",
    "        print(\"Number of training jobs with valid objective: %d\" % len(df))\n",
    "        print({\"lowest\": min(df[\"FinalObjectiveValue\"]), \"highest\": max(df[\"FinalObjectiveValue\"])})\n",
    "        pd.set_option(\"display.max_colwidth\", -1)  # Don't truncate TrainingJobName\n",
    "    else:\n",
    "        print(\"No training jobs have reported valid results yet.\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the best trained model\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same type of instance that we used to train, because usually for inference, less compute power is needed than for training, and in addition, instance endpoints will be up and running for long, it's advisable to choose a cheaper instance for inference.\n",
    "\n",
    "- `ml.p3.2xlarge` - deliver high performance compute in the cloud with up to 8 NVIDIA® V100 Tensor Core GPUs and up to 100 `Gbps` of networking throughput for machine learning and HPC applications.\n",
    "- `ml.g4dn.xlarge` - the industry’s most cost-effective and versatile GPU instances for deploying machine learning models such as image classification, object detection, and speech recognition, and for graphics-intensive applications such as remote graphics workstations, game streaming, and graphics rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-05-10 21:12:40 Starting - Preparing the instances for training\n",
      "2022-05-10 21:12:40 Downloading - Downloading input data\n",
      "2022-05-10 21:12:40 Training - Training image download completed. Training in progress.\n",
      "2022-05-10 21:12:40 Uploading - Uploading generated training model\n",
      "2022-05-10 21:12:40 Completed - Training job completed\n",
      "--------!"
     ]
    }
   ],
   "source": [
    "predictor = tuner.deploy(1, \"ml.p3.2xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence):\n",
    "    result = predictor.predict({\"inputs\": sentence})\n",
    "    index = int(result[0][\"label\"].split(\"LABEL_\")[1])\n",
    "    print(categories[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n",
      "recreational\n",
      "science\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"The modem is an internal AT/(E)ISA 8-bit card (just a little longer than a half-card).\",\n",
    "    \"In the cage I usually wave to bikers.  They usually don't wave back.  My wife thinks it's strange but I don't care.\",\n",
    "    \"Voyager has the unusual luck to be on a stable trajectory out of the solar system.\",\n",
    "]\n",
    "\n",
    "# using the same processing logic that we used during data preparation for training\n",
    "processed_sentences = process_text(sentences)\n",
    "\n",
    "for sentence in processed_sentences:\n",
    "    predict_sentence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "Endpoints should be deleted when no longer in use, since (per the [SageMaker pricing page](https://aws.amazon.com/sagemaker/pricing/)) they're billed by time deployed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
